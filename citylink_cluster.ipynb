{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import jieba\n",
    "import pprint\n",
    "import re, string\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from smart_open import open # for transparently opening compressed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus class for getting one document at a time from the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"Corpus that handles one document at a time\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_path, file_list, dictionary, stop_list, city_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path - root path to the website files\n",
    "            file_list - list of website files\n",
    "            dictionary - mapping from words to ids\n",
    "            stop_list - chinese stopwords set\n",
    "            city_list - chinese city set\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "        self.dictionary = dictionary\n",
    "        self.stop_list = stop_list\n",
    "        self.city_list = city_list\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for filename in self.file_list:\n",
    "            with open(self.root_path + filename, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    words = self._process(line)\n",
    "                    if not words or len(words) < 2:  # less than 2 words won't contain 2 cities\n",
    "                        continue\n",
    "                    words, cities = self._retrieve_cities(words)\n",
    "                    if len(cities) < 2:  # less than 2 cities won't composite a link\n",
    "                        continue\n",
    "#                     yield {'words': self.dictionary.doc2bow(words), 'cities': cities}\n",
    "                    yield {'words': words, 'cities': cities}\n",
    "                    \n",
    "    def _process(self, line):\n",
    "        # drop meta-info\n",
    "        if line == '' or line.startswith('\\r') or line.startswith('WARC') or line.startswith('Content'):\n",
    "            return\n",
    "        # drop alphabetic characters\n",
    "        line = re.sub(r'[a-zA-Z]', '', line)\n",
    "        # drop digits and punctuations\n",
    "        line = re.sub('[%s]' % (string.punctuation + string.digits), '', line)\n",
    "        # drop empty line\n",
    "        if line == '\\r':\n",
    "            return\n",
    "        # segment the sentence using jieba\n",
    "        words = ' '.join(jieba.cut(line, cut_all=False)).split(' ')\n",
    "        # drop stopwords\n",
    "        words = [word for word in words if word not in self.stop_list]\n",
    "        return words\n",
    "    \n",
    "    def _retrieve_cities(self, words):\n",
    "        \"\"\"Caution: cities are removed from the document because cities are not supposed to related to any category\n",
    "        \"\"\"\n",
    "        cities = []\n",
    "        indices = []\n",
    "        for idx, word in enumerate(words):\n",
    "            if word in self.city_list:\n",
    "                cities.append(word)\n",
    "                indices.append(idx)\n",
    "        # remove cities from the document\n",
    "        for idx in indices[::-1]:\n",
    "            del words[idx]\n",
    "        return words, cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that calculates the frequency of every category in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyCalculator:\n",
    "    \"\"\"Calculate frequency of every category in a document by compare the similarities betweeen each word and the keyword\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wv, nclass, keywords, th):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            wv - word vectors\n",
    "            nclass - number of categories\n",
    "            keywords - 文化，经济，体育等. Nested list. [[经济,金融...],[科技,互联网...]...], 一共nclass类\n",
    "            th - similarity threshold. Similarites Under the threshold will be discarded.\n",
    "        \"\"\"\n",
    "        self.wv = wv\n",
    "        self.nclass = nclass\n",
    "        self.keywords = keywords\n",
    "        self.th = th\n",
    "    \n",
    "    def calc(self, words):\n",
    "        \"\"\"Calculate frequency and return it\"\"\"\n",
    "        freq = {i: 0 for i in range(1, self.nclass + 1)}\n",
    "        for word in words:\n",
    "            if word not in self.wv:\n",
    "                continue\n",
    "            for i, category in enumerate(self.keywords, 1):\n",
    "                for key in category:\n",
    "                    if self.wv.similarity(word, key) > self.th:\n",
    "                        freq[i] += 1\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that converts frequency of every document to frequency of two city link in the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8824330 unique tokens: ['汪儒', '老李太太熏酱', '名侦探柯南剧场版15', '安佩佩', '雲端計算']...)\n"
     ]
    }
   ],
   "source": [
    "if 'dict.dict' in os.listdir('../dict'):\n",
    "    dictionary = corpora.Dictionary().load('../dict/dict.dict')  # already processed from embedding file\n",
    "else:\n",
    "    texts = []\n",
    "    with open('../embedding/Tencent_AILab_ChineseEmbedding.txt') as f:\n",
    "        skip_head = True\n",
    "        for line in f:\n",
    "            if skip_head:\n",
    "                skip_head = False\n",
    "                continue\n",
    "            else:\n",
    "                texts.append(line.split(' ')[0])\n",
    "    dictionary = corpora.Dictionary([texts])\n",
    "    dictionary.save('../dict/dict.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '../embedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the stopwords, city names and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "with open('resources/stopwords_zh.txt') as f:\n",
    "    for line in f:\n",
    "        stop_list.append(line[:-1])\n",
    "stop_list = set(stop_list)\n",
    "\n",
    "city_list = []\n",
    "with open('resources/China_Cities_Coordinates_CHN_ENG.csv') as f:\n",
    "    skip_head = True\n",
    "    for line in f:\n",
    "        if skip_head:\n",
    "            skip_head = False\n",
    "            continue\n",
    "        else:\n",
    "            city_list.append(line.split(',')[0])\n",
    "city_list = set(city_list)\n",
    "\n",
    "nclass = 7\n",
    "keywords = [[] for _ in range(nclass)]\n",
    "with open('resources/keywords.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        for i, category in enumerate(line.split(',')):\n",
    "            if category != '' and category in wv_from_text:\n",
    "                keywords[i].append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the corpus and frequency calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f for f in os.listdir('../webdata') if f.startswith('part-')][:1]\n",
    "my_corpus = MyCorpus('../webdata/', file_list, dictionary, stop_list, city_list)\n",
    "freq_calc = FrequencyCalculator(wv_from_text, nclass=nclass, keywords=keywords, th=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main run part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 17, 2: 5, 3: 1, 4: 4, 5: 6, 6: 2, 7: 0}\n",
      "{1: 13, 2: 6, 3: 4, 4: 7, 5: 5, 6: 5, 7: 0}\n",
      "{1: 11, 2: 7, 3: 4, 4: 2, 5: 8, 6: 12, 7: 0}\n",
      "{1: 4, 2: 12, 3: 1, 4: 2, 5: 3, 6: 1, 7: 16}\n",
      "{1: 20, 2: 12, 3: 0, 4: 1, 5: 0, 6: 8, 7: 0}\n",
      "{1: 15, 2: 9, 3: 0, 4: 1, 5: 0, 6: 5, 7: 0}\n",
      "{1: 8, 2: 3, 3: 3, 4: 2, 5: 2, 6: 4, 7: 0}\n",
      "{1: 3, 2: 3, 3: 3, 4: 7, 5: 13, 6: 5, 7: 0}\n",
      "{1: 41, 2: 80, 3: 1, 4: 19, 5: 7, 6: 80, 7: 14}\n",
      "{1: 19, 2: 48, 3: 0, 4: 0, 5: 6, 6: 63, 7: 4}\n",
      "{1: 26, 2: 48, 3: 0, 4: 4, 5: 30, 6: 63, 7: 4}\n",
      "{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: 0}\n",
      "{1: 68, 2: 75, 3: 3, 4: 11, 5: 8, 6: 23, 7: 2}\n",
      "{1: 66, 2: 80, 3: 2, 4: 7, 5: 50, 6: 25, 7: 1}\n",
      "{1: 5, 2: 2, 3: 4, 4: 2, 5: 1, 6: 0, 7: 0}\n",
      "{1: 6, 2: 0, 3: 0, 4: 0, 5: 1, 6: 8, 7: 0}\n",
      "{1: 2, 2: 4, 3: 4, 4: 0, 5: 4, 6: 3, 7: 0}\n",
      "{1: 18, 2: 5, 3: 0, 4: 5, 5: 15, 6: 8, 7: 0}\n",
      "{1: 5, 2: 3, 3: 2, 4: 2, 5: 3, 6: 3, 7: 0}\n",
      "{1: 0, 2: 1, 3: 1, 4: 5, 5: 38, 6: 0, 7: 0}\n",
      "{1: 2, 2: 3, 3: 17, 4: 9, 5: 4, 6: 0, 7: 0}\n"
     ]
    }
   ],
   "source": [
    "frequency = {i: 0 for i in range(1, nclass + 1)}  # final frequency\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "cnt = 0\n",
    "for document in my_corpus:\n",
    "    if cnt > 20:\n",
    "        break\n",
    "    cnt += 1\n",
    "    _freq = freq_calc.calc(document[\"words\"])\n",
    "    print(_freq)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 documents (websites) in total. 128.19871544837952 (avg: 6.1047007356371195) seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "print('{} documents (websites) in total. {} (avg: {}) seconds elapsed.'.format(cnt, end - start, (end - start) / 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the keywords with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['经济', '金融', '公司', '产业', '金融', '证券', '理财', '银行', '基金', '保险', '融资', '财政', '投资', '服务', '钱庄', '会计', '管理', '审计', '消费', '市场', '利率', '财富', '信托', '外汇', '期货', '债券', '商品', '价值', '货币', '成本', '市场', '信用', '边际', '搭便车', '马太效应', '帕累托', '供求', '红利', '房地产', '媒体', '科技', '互联网', '通信', '区块链', '人工智能', '创业', '创新', '数码', '技术', '生产', '效率', '理工', '信息', '电信', '计算机', '量子', '数据结构', '软件', '流量', '神经网络', '大数据', '识别', '计算机图像', '计算机视觉', '图像处理', '数据处理', '设备', '媒体', '腾讯', '阿里', '字节跳动', '谷歌', '研发', '华为', '苹果', '爱立信', '智能', '芯片', '云服务', '在线服务', '电信', '法律', '政治', '政府', '共产', '资本主义', '政党', '权力', '权利', '义务', '主义', '民主', '否决权', '世俗主义', '乱世', '列强', '合法性', '社会', '宪法', '宪政', '右派', '左派', '盛世', '中国梦', '三个代表', '科学发展观', '小康', '毛泽东', '邓小平', '江泽民', '胡锦涛', '习近平', '官员', '阶级', '马克思', '列宁', '选角', '治理', '改革', '开放', '领导', '主席', '总统', '奥巴马', '特朗普', '希拉里', '文学', '教育', '艺术', '收藏', '思想', '民俗', '农家乐', '出国', '留学', '公开课', '学校', '大学', '高考', '中小学', '商学院', '考研', '博士', '科研', '英语', '高校', '博物馆', '展览', '社会', '文化', '传统', '戏剧', '地理', '自由行', '背包客', '领队', '携程', '马蜂窝', '攻略', '游记', '玩乐', '故宫', '西湖', '青城山', '娱乐', '明星', '电影', '电视剧', '综艺', '音乐', '视频', '戏剧', '红人', '韩娱', '好莱坞', '奥斯卡', '篮球', '足球', '体育', '运动', '锻炼', '训练', '竞技', '活动', '媒体', '舞蹈', '表演', '西甲', '德甲', '英超', '世界杯', '奥运会', '体操', '足协', '赛车', '高尔夫', '网球', '法网', '直播', '哔哩哔哩', '时尚', '旅游', '风俗', '流行', '刘德华', '周杰伦', '韩红', '餐饮', '服务业', '第二产业', '工业', '污染', '企业', '制造', '手工业', '化学工业', '夕阳产业', '安全生产', '工商业', '采矿', '造船', '重工业', '土木工程', '汽车', '煤炭', '电力', '能源', '钢铁', '造船', '纺织', '建筑业', '劳动密集型', '资源密集型', '燃气', '加工', '组装', '仓储', '运输', '石油', '机械', '仪器', '材料工程', '航海', '水利工程', '农业', '农田', '种植业', '畜牧业', '渔业', '食品', '种植', '养殖', '耕种', '森林', '野兽', '猪肉', '水产', '家禽']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 1, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 6, 1, 6, 6, 1, 6,\n",
       "       1, 1, 1, 1, 0, 6, 1, 6, 6, 1, 1, 4, 6, 6, 1, 6, 1, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 0, 6, 4, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 5, 2, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 4, 6, 4, 4, 4, 4, 4, 4, 6, 5, 4, 4,\n",
       "       4, 3, 6, 4, 4, 4, 4, 4, 4, 4, 6, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5,\n",
       "       3, 3, 3, 3, 3, 3, 3, 4, 6, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       6, 6, 5, 4, 4, 5, 5, 5, 5, 6, 2, 2, 2, 0, 6, 0, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 4, 4, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = []\n",
    "for cate in keywords:\n",
    "    keys.extend(cate)\n",
    "print(keys)\n",
    "\n",
    "X = np.array([wv_from_text[k] for k in keys])\n",
    "kmeans = KMeans(n_clusters=7, random_state=0, verbose=1).fit(X)\n",
    "kmeans.labels_[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch (not important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 1. Dictionary -->\n",
    "<!-- 2. Stop words -->\n",
    "<!-- 3. Remove without city -->\n",
    "<!-- 4. Store city links -->\n",
    "5. Count words related with different category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('及体育', 0.745394229888916),\n",
       " ('以及体育', 0.7444100379943848),\n",
       " ('体育传媒', 0.7442153096199036),\n",
       " ('体育科技', 0.7429567575454712),\n",
       " ('包括体育', 0.7428886294364929),\n",
       " ('体育方面', 0.7395599484443665),\n",
       " ('体育领域', 0.7245116233825684),\n",
       " ('体育产业', 0.7185760736465454),\n",
       " ('体育相关', 0.7166426181793213),\n",
       " ('娱乐体育', 0.7142459750175476)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text.similar_by_word('体育')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5286123"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text.similarity('体育', '金融')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
