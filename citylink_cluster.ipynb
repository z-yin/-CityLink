{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import pprint\n",
    "import re, string\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from smart_open import open # for transparently opening compressed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus class for getting one document at a time from the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"Corpus that handles one document at a time\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_path, file_list, dictionary, stop_list, city_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path - root path to the website files\n",
    "            file_list - list of website files\n",
    "            dictionary - mapping from words to ids\n",
    "            stop_list - chinese stopwords set\n",
    "            city_list - chinese city set\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "        self.dictionary = dictionary\n",
    "        self.stop_list = stop_list\n",
    "        self.city_list = city_list\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for filename in self.file_list:\n",
    "            with open(self.root_path + filename, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    words = self._process(line)\n",
    "                    if not words or len(words) < 2:  # less than 2 words won't contain 2 cities\n",
    "                        continue\n",
    "                    words, cities = self._retrieve_cities(words)\n",
    "                    if len(cities) < 2:  # less than 2 cities won't composite a link\n",
    "                        continue\n",
    "#                     yield {'words': self.dictionary.doc2bow(words), 'cities': cities}\n",
    "                    yield {'words': words, 'cities': cities}\n",
    "                    \n",
    "    def _process(self, line):\n",
    "        # drop meta-info\n",
    "        if line == '' or line.startswith('\\r') or line.startswith('WARC') or line.startswith('Content'):\n",
    "            return\n",
    "        # drop alphabetic characters\n",
    "        line = re.sub(r'[a-zA-Z]', '', line)\n",
    "        # drop digits and punctuations\n",
    "        line = re.sub('[%s]' % (string.punctuation + string.digits), '', line)\n",
    "        # drop empty line\n",
    "        if line == '\\r':\n",
    "            return\n",
    "        # segment the sentence using jieba\n",
    "        words = ' '.join(jieba.cut(line, cut_all=False)).split(' ')\n",
    "        # drop stopwords\n",
    "        words = [word for word in words if word not in self.stop_list]\n",
    "        return words\n",
    "    \n",
    "    def _retrieve_cities(self, words):\n",
    "        \"\"\"Caution: cities are removed from the document because cities are not supposed to related to any category\n",
    "        \"\"\"\n",
    "        cities = []\n",
    "        indices = []\n",
    "        for idx, word in enumerate(words):\n",
    "            if word in self.city_list:\n",
    "                cities.append(word)\n",
    "                indices.append(idx)\n",
    "        # remove cities from the document\n",
    "        for idx in indices[::-1]:\n",
    "            del words[idx]\n",
    "        return words, cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that calculates the frequency of every category in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyCalculator:\n",
    "    \"\"\"Calculate frequency of every category in a document by compare the similarities betweeen each word and the keyword\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wv, nclass, keywords, th):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            wv - word vectors\n",
    "            nclass - number of categories\n",
    "            keywords - 文化，经济，体育等. Nested list. [[经济,金融...],[科技,互联网...]...], 一共nclass类\n",
    "            th - similarity threshold. Similarites Under the threshold will be discarded.\n",
    "        \"\"\"\n",
    "        self.wv = wv\n",
    "        self.nclass = nclass\n",
    "        self.freq = {i: 0 for i in range(1, nclass + 1)}\n",
    "        self.keywords = keywords\n",
    "        self.th = th\n",
    "    \n",
    "    def calc(self, words):\n",
    "        \"\"\"Calculate frequency and return it\"\"\"\n",
    "        for word in words:\n",
    "            if word not in self.wv:\n",
    "                continue\n",
    "            for i, category in enumerate(self.keywords, 1):\n",
    "                for key in category:\n",
    "                    if self.wv.similarity(word, key) > self.th:\n",
    "                        self.freq[i] += 1\n",
    "        return self.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that converts frequency of every document to frequency of two city link in the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8824330 unique tokens: ['汪儒', '老李太太熏酱', '名侦探柯南剧场版15', '安佩佩', '雲端計算']...)\n"
     ]
    }
   ],
   "source": [
    "if 'dict.dict' in os.listdir('../dict'):\n",
    "    dictionary = corpora.Dictionary().load('../dict/dict.dict')  # already processed from embedding file\n",
    "else:\n",
    "    texts = []\n",
    "    with open('../embedding/Tencent_AILab_ChineseEmbedding.txt') as f:\n",
    "        skip_head = True\n",
    "        for line in f:\n",
    "            if skip_head:\n",
    "                skip_head = False\n",
    "                continue\n",
    "            else:\n",
    "                texts.append(line.split(' ')[0])\n",
    "    dictionary = corpora.Dictionary([texts])\n",
    "    dictionary.save('../dict/dict.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '../embedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the stopwords, city names and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "with open('resources/stopwords_zh.txt') as f:\n",
    "    for line in f:\n",
    "        stop_list.append(line[:-1])\n",
    "stop_list = set(stop_list)\n",
    "\n",
    "city_list = []\n",
    "with open('resources/China_Cities_Coordinates_CHN_ENG.csv') as f:\n",
    "    skip_head = True\n",
    "    for line in f:\n",
    "        if skip_head:\n",
    "            skip_head = False\n",
    "            continue\n",
    "        else:\n",
    "            city_list.append(line.split(',')[0])\n",
    "city_list = set(city_list)\n",
    "\n",
    "nclass = 7\n",
    "keywords = [[] for _ in range(nclass)]\n",
    "with open('resources/keywords.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        for i, category in enumerate(line.split(',')):\n",
    "            if category != '' and category in wv_from_text:\n",
    "                keywords[i].append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the corpus and frequency calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f for f in os.listdir('../webdata') if f.startswith('part-')][:1]\n",
    "my_corpus = MyCorpus('../webdata/', file_list, dictionary, stop_list, city_list)\n",
    "freq_calc = FrequencyCalculator(wv_from_text, nclass=nclass, keywords=keywords, th=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main run part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 133, 2: 95, 3: 17, 4: 32, 5: 45, 6: 73, 7: 20}\n",
      "{1: 146, 2: 101, 3: 21, 4: 39, 5: 50, 6: 78, 7: 20}\n",
      "{1: 157, 2: 108, 3: 25, 4: 41, 5: 58, 6: 90, 7: 20}\n",
      "{1: 161, 2: 120, 3: 26, 4: 43, 5: 61, 6: 91, 7: 36}\n",
      "{1: 181, 2: 132, 3: 26, 4: 44, 5: 61, 6: 99, 7: 36}\n",
      "{1: 196, 2: 141, 3: 26, 4: 45, 5: 61, 6: 104, 7: 36}\n",
      "{1: 204, 2: 144, 3: 29, 4: 47, 5: 63, 6: 108, 7: 36}\n",
      "{1: 207, 2: 147, 3: 32, 4: 54, 5: 76, 6: 113, 7: 36}\n",
      "{1: 248, 2: 227, 3: 33, 4: 73, 5: 83, 6: 193, 7: 50}\n",
      "{1: 267, 2: 275, 3: 33, 4: 73, 5: 89, 6: 256, 7: 54}\n",
      "{1: 293, 2: 323, 3: 33, 4: 77, 5: 119, 6: 319, 7: 58}\n",
      "{1: 293, 2: 323, 3: 33, 4: 77, 5: 119, 6: 320, 7: 58}\n",
      "{1: 361, 2: 398, 3: 36, 4: 88, 5: 127, 6: 343, 7: 60}\n",
      "{1: 427, 2: 478, 3: 38, 4: 95, 5: 177, 6: 368, 7: 61}\n",
      "{1: 432, 2: 480, 3: 42, 4: 97, 5: 178, 6: 368, 7: 61}\n",
      "{1: 438, 2: 480, 3: 42, 4: 97, 5: 179, 6: 376, 7: 61}\n",
      "{1: 440, 2: 484, 3: 46, 4: 97, 5: 183, 6: 379, 7: 61}\n",
      "{1: 458, 2: 489, 3: 46, 4: 102, 5: 198, 6: 387, 7: 61}\n",
      "{1: 463, 2: 492, 3: 48, 4: 104, 5: 201, 6: 390, 7: 61}\n",
      "{1: 463, 2: 493, 3: 49, 4: 109, 5: 239, 6: 390, 7: 61}\n"
     ]
    }
   ],
   "source": [
    "frequency = {i: 0 for i in range(1, nclass + 1)}  # final frequency\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "cnt = 0\n",
    "for document in my_corpus:\n",
    "    if cnt > 20:\n",
    "        break\n",
    "    cnt += 1\n",
    "    _freq = freq_calc.calc(document[\"words\"])\n",
    "    print(_freq)\n",
    "    \n",
    "end = time.time()\n",
    "print('{} documents (websites) in total. {} (avg: {}) seconds elapsed.'.format(cnt, end - start), (end - start) / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch (not important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 1. Dictionary -->\n",
    "<!-- 2. Stop words -->\n",
    "<!-- 3. Remove without city -->\n",
    "<!-- 4. Store city links -->\n",
    "5. Count words related with different category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('及体育', 0.745394229888916),\n",
       " ('以及体育', 0.7444100379943848),\n",
       " ('体育传媒', 0.7442153096199036),\n",
       " ('体育科技', 0.7429567575454712),\n",
       " ('包括体育', 0.7428886294364929),\n",
       " ('体育方面', 0.7395599484443665),\n",
       " ('体育领域', 0.7245116233825684),\n",
       " ('体育产业', 0.7185760736465454),\n",
       " ('体育相关', 0.7166426181793213),\n",
       " ('娱乐体育', 0.7142459750175476)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text.similar_by_word('体育')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5286123"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text.similarity('体育', '金融')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
