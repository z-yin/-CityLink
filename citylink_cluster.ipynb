{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import jieba\n",
    "import pprint\n",
    "import re, string\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from threading import Semaphore\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from smart_open import open # for transparently opening compressed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus class for getting one document at a time from the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"Corpus that handles one document at a time\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_path, file_list, dictionary, stop_list, city_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path - root path to the website files\n",
    "            file_list - list of website files\n",
    "            dictionary - mapping from words to ids\n",
    "            stop_list - chinese stopwords set\n",
    "            city_list - chinese city set\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "        self.dictionary = dictionary\n",
    "        self.stop_list = stop_list\n",
    "        self.city_list = city_list\n",
    "    \n",
    "    def __iter__(self):\n",
    "        jieba.enable_parallel(8)\n",
    "        for filename in self.file_list:\n",
    "            with open(self.root_path + filename, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    words = self._process(line)\n",
    "                    if not words or len(words) < 2:  # less than 2 words won't contain 2 cities\n",
    "                        continue\n",
    "                    words, cities = self._retrieve_cities(words)\n",
    "                    # get unique cities\n",
    "                    cities = list(set(cities))\n",
    "                    if len(cities) < 2:  # less than 2 cities won't composite a link\n",
    "                        continue\n",
    "#                     yield {'words': self.dictionary.doc2bow(words), 'cities': cities}\n",
    "                    yield {'words': words, 'cities': cities}\n",
    "                    \n",
    "    def _process(self, line):\n",
    "        # drop meta-info\n",
    "        if line == '' or line.startswith('\\r') or line.startswith('WARC') or line.startswith('Content'):\n",
    "            return\n",
    "        # drop alphabetic characters\n",
    "        line = re.sub(r'[a-zA-Z]', '', line)\n",
    "        # drop digits and punctuations\n",
    "        line = re.sub('[%s]' % (string.punctuation + string.digits), '', line)\n",
    "        # drop empty line\n",
    "        if line == '\\r':\n",
    "            return\n",
    "        # segment the sentence using jieba\n",
    "        words = ' '.join(jieba.cut(line, cut_all=False)).split(' ')\n",
    "        # drop stopwords\n",
    "        words = [word for word in words if word not in self.stop_list]\n",
    "        return words\n",
    "    \n",
    "    def _retrieve_cities(self, words):\n",
    "        \"\"\"Caution: cities are removed from the document because cities are not supposed to related to any category\n",
    "        \"\"\"\n",
    "        cities = []\n",
    "        indices = []\n",
    "        for idx, word in enumerate(words):\n",
    "            if word in self.city_list:\n",
    "                cities.append(word)\n",
    "                indices.append(idx)\n",
    "        # remove cities from the document\n",
    "        for idx in indices[::-1]:\n",
    "            del words[idx]\n",
    "        cities = list(set(cities))\n",
    "        return words, cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that calculates the frequency of every category in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyCalculator:\n",
    "    \"\"\"Calculate frequency of every category in a document by compare the similarities betweeen each word and the keyword\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wv, nclass, keywords, th):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            wv - word vectors\n",
    "            nclass - number of categories\n",
    "            keywords - 文化，经济，体育等. Nested list. [[经济,金融...],[科技,互联网...]...], 一共nclass类\n",
    "            th - similarity threshold. Similarites Under the threshold will be discarded.\n",
    "        \"\"\"\n",
    "        self.wv = wv\n",
    "        self.nclass = nclass\n",
    "        self.keywords = keywords\n",
    "        self.th = th\n",
    "    \n",
    "    def calc(self, words):\n",
    "        \"\"\"Calculate frequency and return it\"\"\"\n",
    "        freq = {i: 0 for i in range(1, self.nclass + 1)}\n",
    "        for word in words:\n",
    "            if word not in self.wv:\n",
    "                continue\n",
    "            for i, category in enumerate(self.keywords, 1):\n",
    "                for key in category:\n",
    "                    if self.wv.similarity(word, key) > self.th:\n",
    "                        freq[i] += 1\n",
    "        return freq\n",
    "    \n",
    "    def calc_given_keywords(self, words, expanded_keywords):\n",
    "        \"\"\"Calculate frequency given keywords and return it\"\"\"\n",
    "        freq = np.array([0 for _ in range(expanded_nclass)]) # easy to add up\n",
    "        for word in words:\n",
    "            if word not in self.wv:\n",
    "                continue\n",
    "            for i, category in enumerate(expanded_keywords, 1):\n",
    "                if word in category:\n",
    "                    freq[i-1] += 1 # -1 for the right index\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that converts frequency of every document to frequency of two city link in the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-8b9ee6dfbc02>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8b9ee6dfbc02>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class FrequencyConverter\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class FrequencyConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8824330 unique tokens: ['澹定集', '存势', '先集', '高性能稀土永磁', '零食图片']...)\n"
     ]
    }
   ],
   "source": [
    "if 'dict.dict' in os.listdir('../dict'):\n",
    "    dictionary = corpora.Dictionary().load('../dict/dict.dict')  # already processed from embedding file\n",
    "else:\n",
    "    texts = []\n",
    "    with open('../embedding/Tencent_AILab_ChineseEmbedding.txt') as f:\n",
    "        skip_head = True\n",
    "        for line in f:\n",
    "            if skip_head:\n",
    "                skip_head = False\n",
    "                continue\n",
    "            else:\n",
    "                texts.append(line.split(' ')[0])\n",
    "    dictionary = corpora.Dictionary([texts])\n",
    "    dictionary.save('../dict/dict.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('headlines', 0.7523147463798523),\n",
       " ('best', 0.7397920489311218),\n",
       " ('phones', 0.7249292731285095),\n",
       " ('times', 0.7244877219200134),\n",
       " ('find', 0.7193858623504639),\n",
       " ('simply', 0.7189313769340515),\n",
       " ('looks', 0.7098098993301392),\n",
       " ('places', 0.7097486257553101),\n",
       " ('smartphone', 0.7090697288513184),\n",
       " ('thing', 0.7079541683197021)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'Bin_Tencent_AILab_ChineseEmbedding.bin' not in os.listdir('../embedding'):\n",
    "    embedding_file = '../embedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "    wv = KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "    wv.init_sims(replace=True)\n",
    "    wv.save('../embedding/Bin_Tencent_AILab_ChineseEmbedding.bin')\n",
    "\n",
    "wv = KeyedVectors.load('../embedding/Bin_Tencent_AILab_ChineseEmbedding.bin', mmap='r')\n",
    "wv.vectors_norm = wv.vectors  # prevent recalc of normed vectors\n",
    "wv.most_similar('stuff')  # any word will do: just to page all in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the stopwords, city names and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "with open('resources/stopwords_zh.txt') as f:\n",
    "    for line in f:\n",
    "        stop_list.append(line[:-1])\n",
    "stop_list = set(stop_list)\n",
    "\n",
    "city_list = []\n",
    "with open('resources/China_Cities_Coordinates_CHN_ENG.csv') as f:\n",
    "    skip_head = True\n",
    "    for line in f:\n",
    "        if skip_head:\n",
    "            skip_head = False\n",
    "            continue\n",
    "        else:\n",
    "            city_list.append(line.split(',')[0])\n",
    "city_list = set(city_list)  \n",
    "\n",
    "nclass = 7\n",
    "keywords = [[] for _ in range(nclass)]\n",
    "with open('resources/keywords.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        for i, category in enumerate(line.split(',')):\n",
    "            if category != '' and category in wv:\n",
    "                keywords[i].append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an Expanded Keywords List Based on the Thresold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'expanded_keywords.csv' in os.listdir('resources'): # already expanded, load from saved\n",
    "    expanded_nclass = 7\n",
    "    expanded_keywords = [[] for _ in range(expanded_nclass)]\n",
    "    with open('resources/expanded_keywords.csv') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.replace('\\n', '')\n",
    "            line = line.split(',')\n",
    "            for keyword in line:\n",
    "                expanded_keywords[i].append(keyword)\n",
    "else: # Expand the existing keywords by finding words in the embedding file that are above the threshold\n",
    "    expanded_nclass = 7\n",
    "    expanded_keywords = [[] for _ in range(expanded_nclass)]\n",
    "    start = time.time()\n",
    "    for i, category in enumerate(keywords, 1):\n",
    "        for key in category:\n",
    "            if key not in repeated_keys: # get most similar words to keys whose similarity > threshold\n",
    "                expanded_keywords[i-1].append(key)\n",
    "                closest = wv.most_similar(key)\n",
    "                for sim_pair in closest:\n",
    "                    if sim_pair[1] > 0.8:\n",
    "                        expanded_keywords[i-1].append(sim_pair[0])\n",
    "                    else:\n",
    "                        break\n",
    "    with open('resources/expanded_keywords.csv', \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for category in expanded_keywords:\n",
    "            writer.writerow(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get city links\n",
    "city_l = list(city_list)\n",
    "city_link = {}\n",
    "for i in range(len(city_l)-1):\n",
    "    for j in range(i+1,len(city_l)):\n",
    "        city_link[(city_l[i], city_l[j])] = np.array([0 for _ in range(expanded_nclass)]) # easy to add up\n",
    "city_link[('许昌', '郴州')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the corpus and frequency calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f for f in os.listdir('../webdata') if f.startswith('part-')][:8]\n",
    "my_corpus = MyCorpus('../webdata/', file_list, dictionary, stop_list, city_list)\n",
    "freq_calc = FrequencyCalculator(wv, nclass=nclass, keywords=keywords, th=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main run part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59401 46701 12425 32204 31928 30257  4330]\n"
     ]
    }
   ],
   "source": [
    "# frequency = {i: 0 for i in range(1, nclass + 1)}  # final frequency\n",
    "frequency = np.array([0 for _ in range(expanded_nclass)]) # easy to add up\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "cnt = 0\n",
    "for document in my_corpus:\n",
    "#     if cnt > 18000:\n",
    "#         break\n",
    "    cnt += 1\n",
    "    _freq = freq_calc.calc_given_keywords(document[\"words\"], expanded_keywords)\n",
    "    # update final frequency\n",
    "    frequency += _freq\n",
    "    # update city_link\n",
    "    current_city_list = document[\"cities\"]\n",
    "    for i in range(len(current_city_list)): # combine \"A-B\" and \"B-A\" city pairs\n",
    "        for j in range(len(current_city_list)):\n",
    "            if (current_city_list[i], current_city_list[j]) in city_link:\n",
    "                city_link[(current_city_list[i], current_city_list[j])] += _freq\n",
    "            if (current_city_list[j], current_city_list[i]) in city_link:\n",
    "                city_link[(current_city_list[j], current_city_list[i])] += _freq\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(frequency) # total frequency\n",
    "\n",
    "with open('results/city_link_frequency.csv', \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(('City1','City2','经济','科技','法律','文学','娱乐','第二产业','农业')) # first row as header\n",
    "    for key, value in city_link.items():\n",
    "        writer.writerow((key[0], key[1], value[0], value[1], value[2], value[3], value[4], value[5], value[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4208 documents (websites) in total. 295.45833134651184 (avg: 0.07021348178386688) seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "print('{} documents (websites) in total. {} (avg: {}) seconds elapsed.'.format(cnt, end - start, (end - start) / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the keywords with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for cate in keywords:\n",
    "    keys.extend(cate)\n",
    "print(keys)\n",
    "\n",
    "X = np.array([wv[k] for k in keys])\n",
    "kmeans = KMeans(n_clusters=7, random_state=0, verbose=1).fit(X)\n",
    "# kmeans.labels_[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use t-SNE to project vectors on a 2-D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and visualize the embedding vectors\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a Common Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9465372562408447\n"
     ]
    }
   ],
   "source": [
    "max_similarity = -1\n",
    "repeated_keys = []\n",
    "for i in range(6):\n",
    "    for key in keywords[i]:\n",
    "        for j in range(i+1,7):\n",
    "            for oppo_key in keywords[j]:\n",
    "                if key != oppo_key:\n",
    "                    if wv.similarity(key, oppo_key) > max_similarity:\n",
    "                        max_similarity = wv.similarity(key, oppo_key)\n",
    "                        key1 = key\n",
    "                        key2 = oppo_key\n",
    "                else:\n",
    "                    repeated_keys.append(key)\n",
    "repeated_keys = set(repeated_keys) # used for later to filter repeated keywords\n",
    "midpoint = (wv[key1] + wv[key2])/2 # middle vector of the two words\n",
    "threshold = wv.most_similar(positive=[midpoint])[0][1]\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scratch (not important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 1. Dictionary -->\n",
    "<!-- 2. Stop words -->\n",
    "<!-- 3. Remove without city -->\n",
    "<!-- 4. Store city links -->\n",
    "5. Count words related with different category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('其他国家', 0.765122652053833),\n",
       " ('国家', 0.7572988271713257),\n",
       " ('美国', 0.7531553506851196),\n",
       " ('我国', 0.7485135793685913),\n",
       " ('日本', 0.7391062378883362),\n",
       " ('国内', 0.7384742498397827),\n",
       " ('大国', 0.7368021011352539),\n",
       " ('其它国家', 0.7294522523880005),\n",
       " ('印度', 0.7231327295303345),\n",
       " ('亚洲各国', 0.7223320007324219),\n",
       " ('外国', 0.7215993404388428),\n",
       " ('西方国家', 0.7174001932144165),\n",
       " ('美国和日本', 0.7170973420143127),\n",
       " ('国家的', 0.7164299488067627),\n",
       " ('全球', 0.7161116600036621),\n",
       " ('世界', 0.7092742919921875),\n",
       " ('美国和欧洲', 0.7090434432029724),\n",
       " ('世界各国', 0.7055490016937256),\n",
       " ('本国', 0.7048956751823425),\n",
       " ('东亚地区', 0.7015669345855713),\n",
       " ('中美', 0.7001239061355591),\n",
       " ('美国日本', 0.6993341445922852),\n",
       " ('周边国家', 0.6977828145027161),\n",
       " ('发达国家', 0.6977471709251404),\n",
       " ('越南', 0.6975182890892029),\n",
       " ('欧美国家', 0.6964695453643799),\n",
       " ('日本，日本', 0.6953955888748169),\n",
       " ('俄罗斯', 0.6944745779037476),\n",
       " ('别国', 0.6911125183105469),\n",
       " ('其他国家和地区', 0.6910572648048401),\n",
       " ('其他亚洲国家', 0.6908308267593384),\n",
       " ('各国', 0.6906349658966064),\n",
       " ('世界大国', 0.6904644966125488),\n",
       " ('国', 0.6897127032279968),\n",
       " ('中美两国', 0.6888139247894287),\n",
       " ('欧美等国', 0.68694007396698),\n",
       " ('经济大国', 0.6847956776618958),\n",
       " ('日本和印度', 0.682314932346344),\n",
       " ('韩国和日本', 0.6811762452125549),\n",
       " ('华夏', 0.6809737682342529),\n",
       " ('亚洲', 0.6776257753372192),\n",
       " ('欧洲国家', 0.6775132417678833),\n",
       " ('巴基斯坦', 0.6772482991218567),\n",
       " ('日本等国', 0.6762583255767822),\n",
       " ('东南亚', 0.675677478313446),\n",
       " ('世界强国', 0.6754803657531738),\n",
       " ('世界其他地区', 0.6740224957466125),\n",
       " ('领先世界', 0.6727827787399292),\n",
       " ('东亚国家', 0.6720367670059204)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in wv.similar_by_word('中国', 100) if '中国' not in i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4333036"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('拓荒者', '中国')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
