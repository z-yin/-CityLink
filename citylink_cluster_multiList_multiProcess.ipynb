{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import jieba\n",
    "import pprint\n",
    "import re, string\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "# from threading import Semaphore, Thread, Event # for multi-processing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from smart_open import open # for transparently opening compressed files\n",
    "import multiprocessing as mp # pool, pipe, process for multi-processing\n",
    "from itertools import repeat, islice # itertools.islice, for slicing generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Number of Processes 8\n",
      "Enter Number of Files 8\n"
     ]
    }
   ],
   "source": [
    "NCORE = input(\"Enter Number of Processes\")\n",
    "NUM_FILES = input(\"Enter Number of Files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Process Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_given_keywords(words, expanded_nclass, expanded_keywords):\n",
    "    \"\"\"Calculate frequency given keywords and return it\"\"\"\n",
    "    freq = np.array([0 for _ in range(expanded_nclass)]) # easy to add up\n",
    "    for word in words:\n",
    "        for i, category in enumerate(expanded_keywords, 1):\n",
    "            if word in category:\n",
    "                freq[i-1] += 1 # -1 for the right index\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_process(document, city_link, expanded_keywords):\n",
    "    words = document[0]\n",
    "    cities = document[1]\n",
    "    # frequency in the format: array([0, 0, 0, 0, 0, 0, 0]), easy to add up\n",
    "    _freq = calc_given_keywords(words, len(expanded_keywords), expanded_keywords)\n",
    "    # combine \"A-B\" and \"B-A\" city pairs\n",
    "    for i in range(len(cities)): \n",
    "        for j in range(len(cities)):\n",
    "            if i != j:\n",
    "                if (cities[i], cities[j]) in city_link:\n",
    "                    city_link[(cities[i], cities[j])] += _freq\n",
    "                if (cities[j], cities[i]) in city_link:\n",
    "                    city_link[(cities[j], cities[i])] += _freq\n",
    "    return city_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f for f in os.listdir('../webdata') if f.startswith('part-')][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8824330 unique tokens: ['经自贸', 'dcfever', '智能电池系统', '观阴大湿', '永康车站']...)\n"
     ]
    }
   ],
   "source": [
    "print('loading dictionary...')\n",
    "if 'dict.dict' in os.listdir('../dict'):\n",
    "    dictionary = corpora.Dictionary().load('../dict/dict.dict')  # already processed from embedding file\n",
    "else:\n",
    "    texts = []\n",
    "    with open('../embedding/Tencent_AILab_ChineseEmbedding.txt') as f:\n",
    "        skip_head = True\n",
    "        for line in f:\n",
    "            if skip_head:\n",
    "                skip_head = False\n",
    "                continue\n",
    "            else:\n",
    "                texts.append(line.split(' ')[0])\n",
    "    dictionary = corpora.Dictionary([texts])\n",
    "    dictionary.save('../dict/dict.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "with open('resources/stopwords_zh.txt') as f:\n",
    "    for line in f:\n",
    "        stop_list.append(line[:-1])\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### City List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = []\n",
    "with open('resources/China_Cities_Coordinates_CHN_ENG.csv') as f:\n",
    "    skip_head = True\n",
    "    for line in f:\n",
    "        if skip_head:\n",
    "            skip_head = False\n",
    "            continue\n",
    "        else:\n",
    "            city_list.append(line.split(',')[0])\n",
    "city_list = list(set(city_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save 'Bin_Tencent_AILab_ChineseEmbedding.bin' in '../embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Bin_Tencent_AILab_ChineseEmbedding.bin' not in os.listdir('../embedding'):\n",
    "    embedding_file = '../embedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "    wv = KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "    wv.init_sims(replace=True)\n",
    "    wv.save('../embedding/Bin_Tencent_AILab_ChineseEmbedding.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "wv = KeyedVectors.load('../embedding/Bin_Tencent_AILab_ChineseEmbedding.bin', mmap='r')\n",
    "wv.vectors_norm = wv.vectors  # prevent recalc of normed vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save 'expanded_keywords.csv' in 'resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'expanded_keywords.csv' not in os.listdir('resources'):\n",
    "    # Expand the existing keywords by finding words in the embedding file that are above the threshold\n",
    "    '''load keywords'''\n",
    "    nclass = 7\n",
    "    keywords = [[] for _ in range(nclass)]\n",
    "    with open('resources/keywords.csv') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n', '')\n",
    "            for i, category in enumerate(line.split(',')):\n",
    "                if category != '' and category in wv:\n",
    "                    keywords[i].append(category)\n",
    "\n",
    "    '''save expanded keywords'''\n",
    "    expanded_nclass = 7\n",
    "    expanded_keywords = [[] for _ in range(expanded_nclass)]\n",
    "    for i, category in enumerate(keywords, 1):\n",
    "        for key in category:\n",
    "            # get most similar words to keys whose similarity > threshold\n",
    "            expanded_keywords[i-1].append(key)\n",
    "            closest = wv.most_similar(key)\n",
    "            for sim_pair in closest:\n",
    "                if sim_pair[1] > 0.8:\n",
    "                    expanded_keywords[i-1].append(sim_pair[0])\n",
    "                else:\n",
    "                    break\n",
    "    with open('resources/expanded_keywords.csv', \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for category in expanded_keywords:\n",
    "            writer.writerow(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Expanded Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading keywords...')\n",
    "if 'expanded_keywords.csv' in os.listdir('resources'): # already expanded, load from saved\n",
    "    expanded_nclass = 7\n",
    "    expanded_keywords = [[] for _ in range(expanded_nclass)]\n",
    "    with open('resources/expanded_keywords.csv') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.replace('\\n', '')\n",
    "            line = line.split(',')\n",
    "            for keyword in line:\n",
    "                expanded_keywords[i].append(keyword)\n",
    "else:\n",
    "    print('Error: Expanded keywords not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading {} files...'.format(NUM_FILES))\n",
    "documents = [] # [document[[words],[cities]]]\n",
    "jieba.enable_parallel(NCORE)\n",
    "for filename in file_list[:NUM_FILES]:\n",
    "    with open('../webdata/' + filename, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            document = []\n",
    "            # drop meta-info\n",
    "            if line == '' or line.startswith('\\r') or line.startswith('WARC') or line.startswith('Content'):\n",
    "                continue\n",
    "            # drop alphabetic characters\n",
    "            line = re.sub(r'[a-zA-Z]', '', line)\n",
    "            # drop digits and punctuations\n",
    "            line = re.sub('[%s]' % (string.punctuation + string.digits), '', line)\n",
    "            # drop empty line\n",
    "            if line == '\\r':\n",
    "                continue\n",
    "            # segment the sentence using jieba\n",
    "            words = ' '.join(jieba.cut(line, cut_all=False)).split(' ')\n",
    "            # drop stopwords\n",
    "            words = [word for word in words if word not in stop_list]\n",
    "            if not words or len(words) < 2:  # less than 2 words won't contain 2 cities\n",
    "                continue\n",
    "            cities = []\n",
    "            indices = []\n",
    "            for idx, word in enumerate(words):\n",
    "                if word in city_list:\n",
    "                    cities.append(word)\n",
    "                    indices.append(idx)\n",
    "            # remove cities from the document\n",
    "            for idx in indices[::-1]:\n",
    "                del words[idx]\n",
    "            cities = list(set(cities)) # get unique cities\n",
    "            if len(cities) < 2:  # less than 2 cities won't composite a link\n",
    "                continue\n",
    "            document.append(words)\n",
    "            document.append(cities)\n",
    "            documents.append(document)\n",
    "jieba.disable_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mp.manager (slow)/mp.pool.apply_async (serial) if used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager = mp.Manager()\n",
    "# documents_ref = manager.list(documents)\n",
    "# city_link_ref = manager.dict(city_link)\n",
    "# expanded_keywords_ref = manager.list(expanded_keywords)\n",
    "\n",
    "## Instantiate the pool here\n",
    "# pool = mp.Pool(processes=NCORE)\n",
    "# pool.apply_async(parallel_process, (documents_ref[(NCORE-1)*NDOC:], \n",
    "#                                     city_link_ref, expanded_keywords_ref))\n",
    "# for core in range(NCORE-1):\n",
    "#     pool.apply_async(parallel_process, (documents_ref[core*NDOC:(core+1)*NDOC],\n",
    "#                                         city_link_ref, expanded_keywords_ref))\n",
    "\n",
    "# results = [pool.apply_async(parallel_process, (documents[core*NDOC:(core+1)*NDOC], \n",
    "#                                                city_link, expanded_keywords)).get() \n",
    "#            if core < NCORE-1 else pool.apply_async(parallel_process, (documents[(NCORE-1)*NDOC:], \n",
    "#                                     city_link, expanded_keywords)).get()\n",
    "#           for core in range(NCORE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Run Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise City Link\n",
    "city_link = {}\n",
    "expanded_nclass = len(expanded_keywords)\n",
    "for i in range(len(city_list)-1):\n",
    "    for j in range(i+1,len(city_list)):\n",
    "        city_link[(city_list[i], city_list[j])] = np.array([0 for _ in range(expanded_nclass)]) # easy to add up\n",
    "\n",
    "start = time.time()\n",
    "NDOC = len(documents)//NCORE # num_documents_per_process\n",
    "\n",
    "print('Start {} processes...'.format(NCORE))\n",
    "# Instantiate the pool here\n",
    "pool = mp.Pool(processes=NCORE)\n",
    "result = pool.starmap_async(parallel_process, zip(documents, repeat(city_link), repeat(expanded_keywords)), NDOC)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "result = result.get()\n",
    "\n",
    "print('Get individual result from each process after {} seconds elapsed.'.format(NCORE, time.time() - start))\n",
    "\n",
    "for key in city_link.keys():\n",
    "    saved_previous = np.array([-1 for _ in range(expanded_nclass)])\n",
    "    for i, res in enumerate(result):\n",
    "        if not np.array_equal(saved_previous,res[key]):\n",
    "            saved_previous = res[key]\n",
    "            city_link[key] += res[key]\n",
    "\n",
    "print('Get final results after {} seconds elapsed.'.format(time.time() - start))\n",
    "\n",
    "print('Saving...')\n",
    "with open('results/city_link_frequency_multiThreads.csv', \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(('City1','City2','经济','科技','法律','文学','娱乐','第二产业','农业')) # first row as header\n",
    "    for key, value in city_link.items():\n",
    "        writer.writerow((key[0], key[1], value[0], value[1], value[2], value[3], value[4], value[5], value[6]))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
